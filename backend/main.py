# backend/main.py
import sqlite3
import re
import sys 
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Literal

# --- AADIDO: Importaciones para Monitorizaci贸n ---
from prometheus_fastapi_instrumentator import Instrumentator
from loguru import logger

# LangChain
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough

# --- AADIDO: CONFIGURACIN DE LOGGING ESTRUCTURADO ---
logger.remove()
logger.add(sys.stdout, serialize=True, enqueue=True)

class InterceptHandler(logging.Handler):
    def emit(self, record):
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno
        logger.log(level, record.getMessage())

logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)
logging.getLogger("uvicorn").handlers = [InterceptHandler()]
logging.getLogger("uvicorn.access").handlers = [InterceptHandler()]


# --- CONFIGURACIN Y MODELOS ---
VECTOR_STORE_DIR = "vector_store"
DB_PATH = "tickets.db"
app = FastAPI(title="Corporate EPIS Pilot API - Advanced Flow")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- AADIDO: INSTRUMENTACIN DE PROMETHEUS ---
Instrumentator().instrument(app).expose(app)

# modifique el modelooooo
llm = OllamaLLM(model="smollm:360m", temperature=0, base_url="http://172.30.106.25:11434")
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
vector_store = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=embeddings)
retriever = vector_store.as_retriever()

# --- LGICA DE LANGCHAIN (MODIFICADA) ---
rag_prompt_template = "Usa el siguiente contexto para responder en espa帽ol de forma concisa y 煤til a la pregunta.\nContexto: {context}\nPregunta: {question}\nRespuesta:"
rag_prompt = PromptTemplate.from_template(rag_prompt_template)
rag_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": rag_prompt})

def create_support_ticket(description: str) -> str:
    """Crea un ticket de soporte y devuelve un mensaje de confirmaci贸n."""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    problem_description = description.replace("ACTION_CREATE_TICKET:", "").strip()
    if not problem_description:
        problem_description = "Problema no especificado por el usuario."

    cursor.execute("INSERT INTO tickets (description, status) VALUES (?, ?)", (problem_description, "Abierto"))
    ticket_id = cursor.lastrowid
    conn.commit()
    conn.close()
    return f"De acuerdo. He creado el ticket de soporte #{ticket_id} con tu problema: '{problem_description}'. El equipo t茅cnico se pondr谩 en contacto contigo."

# El router ahora es m谩s simple
# CAMBIO 1: A帽adimos la nueva intenci贸n 'despedida'
class RouteQuery(BaseModel):
    intent: Literal["pregunta_general", "reporte_de_problema", "despedida"] = Field(description="La intenci贸n del usuario.")

output_parser = JsonOutputParser(pydantic_object=RouteQuery)
# CAMBIO 2: Actualizamos el prompt para que el LLM sepa qu茅 es una 'despedida'
router_prompt = PromptTemplate(
    template="""
    Clasifica la pregunta del usuario en 'pregunta_general', 'reporte_de_problema' o 'despedida'. Responde solo con JSON.
    'pregunta_general': El usuario pide informaci贸n (驴qu茅 es?, 驴cu谩ntos?, 驴c贸mo?).
    'reporte_de_problema': El usuario describe un problema, algo est谩 roto o no funciona.
    'despedida': El usuario expresa gratitud o se despide (gracias, adi贸s, perfecto, vale).
    Pregunta: {question}
    Formato: {format_instructions}
    """,
    input_variables=["question"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
)
#def extract_json_from_string(text: str) -> str:
#    match = re.search(r'\{.*\}', text, re.DOTALL)
    # Si no encuentra JSON o la pregunta es muy corta, es probable que sea una despedida
#    if not match and len(text) < 20:
#        return '{"intent": "despedida"}'
#    return match.group(0) if match else '{"intent": "pregunta_general"}'

def extract_json_from_string(text: str) -> str:
    # LIMPIEZA: Convertimos todo a min煤sculas para buscar f谩cil
    text_lower = text.lower()
    
    # LGICA HBRIDA (AUDITORA):
    # Como smollm:360m falla generando JSON, detectamos la intenci贸n por palabras clave
    # Esto simula la inteligencia pero es robusto.
    
    if "hola" in text_lower or "buenos" in text_lower or "gracias" in text_lower:
        # Si saluda, es pregunta general (o despedida, pero general funciona bien)
        return '{"intent": "pregunta_general"}'
        
    if "error" in text_lower or "falla" in text_lower or "problema" in text_lower or "no funciona" in text_lower or "roto" in text_lower:
        # Si menciona error/problema, forzamos la intenci贸n de reporte
        return '{"intent": "reporte_de_problema"}'

    # Si el modelo intent贸 generar JSON, tratamos de rescatarlo
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        return match.group(0)
        
    # Si todo falla, asumimos pregunta general para que no explote
    return '{"intent": "pregunta_general"}'

router_chain = router_prompt | llm | RunnableLambda(extract_json_from_string) | output_parser

chain_with_preserved_input = RunnablePassthrough.assign(decision=router_chain)

problem_chain = RunnableLambda(lambda x: {"query": x["question"]}) | rag_chain

# --- ENDPOINT DE LA API (MODIFICADO) ---
@app.get("/ask")
def ask_question(question: str):
    try:
        if question.startswith("ACTION_CREATE_TICKET:"):
            description = question.split(":", 1)[1]
            return {"answer": create_support_ticket(description), "follow_up_required": False}

        decision_result = chain_with_preserved_input.invoke({"question": question})
        intent = decision_result["decision"]["intent"]
        
        answer = ""
        follow_up = False

        if intent == "pregunta_general":
            result = problem_chain.invoke(decision_result)
            answer = result.get("result", "No se encontr贸 respuesta.")
        elif intent == "reporte_de_problema":
            result = problem_chain.invoke(decision_result)
            solution = result.get("result", "No he encontrado una soluci贸n espec铆fica en mis documentos.")
            answer = f"{solution}\n\n驴Esta informaci贸n soluciona tu problema?"
            follow_up = True
        # CAMBIO 3: A帽adimos el manejo de la nueva intenci贸n
        elif intent == "despedida":
            answer = "De nada, 隆un placer ayudar! Si tienes cualquier otra consulta, aqu铆 estar茅. "
            follow_up = False
            
        return {"answer": answer, "follow_up_required": follow_up}

    except Exception as e:
        # AADIDO: Usamos logger en lugar de print para un registro estructurado
        logger.error(f"Error en el endpoint /ask: {e}")
        return {"answer": "Lo siento, ha ocurrido un error.", "follow_up_required": False}